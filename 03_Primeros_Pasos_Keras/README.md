# Primeros Pasos Keras: Tipos de Problemas Fundamentales

En este apartado se exploran los tres tipos de problemas m谩s comunes en Machine Learning que sirven como introducci贸n pr谩ctica al uso de redes neuronales con Keras:

1.  **Clasificaci贸n Binaria:** Predecir una de dos clases posibles (e.g., s铆/no, 0/1).
2.  **Clasificaci贸n Multiclase:** Predecir una de *N* clases posibles (e.g., clasificar d铆gitos del 0 al 9).
3.  **Regresi贸n:** Predecir un valor continuo (e.g., la tendencia de un precio).

Como ya se ha establecido, el entrenamiento de una red neuronal requiere de cuatro componentes clave:
- **Modelo:** Las capas encadenadas que transforman los datos.
- **Datos de Entrada y Objetivos (Target):** El conjunto de datos de entrenamiento y los resultados esperados.
- **Funci贸n de P茅rdida (Loss Function):** La se帽al de retroalimentaci贸n utilizada para medir el error.
- **Optimizador:** El mecanismo que ajusta los pesos del modelo para minimizar la p茅rdida.

---

## Construcci贸n de la Red: Capas y Unidades

### Arquitectura y Tipos de Capas

La construcci贸n de la red neuronal depende de la estructura del tensor de entrada, el bloque principal es la **capa**.

| Tipo de Datos | Forma del Tensor | Capa Com煤n | Descripci贸n |
| :--- | :--- | :--- | :--- |
| **Vectoriales** | 2D: `(muestras, caracter铆sticas)` | `Dense` | Capa completamente conectada. |
| **Secuenciales** | 3D: `(muestras, tiempo, caracter铆sticas)` | `LSTM` | Capa Recurrente para secuencias. |
| **Im谩genes** | 4D: `(muestras, alto, ancho, canales)` | `Conv2D` | Capa Convolucional. |

Los pesos de las capas son los **tensores aprendibles** que contienen el conocimiento de la red, y son ajustados mediante el descenso de gradiente estoc谩stico.

### Unidades Ocultas (Dimensionalidad de la Representaci贸n)

El argumento clave en las capas **`Dense`** es el **n煤mero de unidades ocultas**.

* Este n煤mero define la **dimensi贸n del espacio de representaci贸n** de la capa, es decir, cu谩nta **capacidad** (libertad para aprender) permitimos que tenga la red.
* Un n煤mero alto de unidades permite aprender representaciones m谩s complejas, pero aumenta el riesgo de **sobreajuste (overfitting)** y el coste computacional.
* La arquitectura central consiste en elegir: cu谩ntas capas encadenar y cu谩ntas unidades ocultas asignar a cada una.

###  Funci贸n de Activaci贸n

Una capa **`Dense`** sin funci贸n de activaci贸n solo realizar铆a una transformaci贸n lineal: $\text{output} = (\mathbf{W} \cdot \mathbf{x} + \mathbf{b})$.

Para que la red pueda aprender transformaciones **no lineales** de los datos (lo cual es esencial para resolver problemas complejos), es necesario aplicar una **funci贸n de activaci贸n no lineal** despu茅s de la operaci贸n lineal.

* **`relu` (Unidad Lineal Rectificada):** Llama a cero los valores negativos. Es la activaci贸n por defecto en muchas capas ocultas.
* **`sigmoid` (Sigmoidea):** Comprime valores arbitrarios en el intervalo **$[0, 1]$**. Esto es 煤til para la capa de salida en problemas de clasificaci贸n binaria, donde los valores se interpretan como **probabilidades**.

---

## Configuraci贸n: P茅rdida y Optimizador

### Funci贸n de P茅rdida y Problemas

Para problemas de clasificaci贸n que arrojan resultados en forma de probabilidad, la funci贸n de p茅rdida m谩s apropiada mide la distancia entre las distribuciones de probabilidad.

* **Clasificaci贸n Binaria:** La mejor opci贸n es la **Entrop铆a Cruzada Binaria (`binary\_crossentropy`)**. Esta m茅trica, proveniente de la Teor铆a de la Informaci贸n, cuantifica la diferencia entre la distribuci贸n de probabilidad predicha por la red y la distribuci贸n de probabilidad verdadera (el objetivo).
* **Optimizador:** El optimizador (e.g., `RMSprop`, `Adam`) es el algoritmo que ajusta los pesos de la red bas谩ndose en la retroalimentaci贸n de la funci贸n de p茅rdida.