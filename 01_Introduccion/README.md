# 01 Introducci贸n y Fundamentos

Este cap铆tulo introduce las definiciones y las diferencias clave entre **Inteligencia Artificial (IA)**, **Machine Learning (ML)**, y **Deep Learning (DL)**, estableciendo el marco conceptual para el resto del libro.

---

## Conceptos Clave del Machine Learning

###  Machine Learning

El ML se centra en algoritmos que aprenden patrones a partir de datos, en lugar de ser programados expl铆citamente.

* **Entrada de datos:** Se proporcionan puntos de datos (im谩genes, archivos, series de tiempo) junto con **ejemplos de resultados esperados** (etiquetas, transcripciones).
* **Funci贸n de P茅rdida (Loss Function):** El mecanismo de medici贸n que cuantifica la distancia entre las predicciones del modelo y los objetivos verdaderos. Act煤a como la **se帽al de retroalimentaci贸n** fundamental para el proceso de aprendizaje.
* **Representaci贸n de Datos:** La forma en que se codifican los datos afecta directamente el rendimiento del modelo. Por ejemplo, para obtener tonos de color, el formato **RGB** puede ser mejor; para la saturaci贸n, el formato **HSV** puede simplificar el problema (cambio de coordenadas).

---

###  Deep Learning

El DL es una subdisciplina de ML que utiliza **Redes Neuronales Profundas** (con muchas capas).

* **Capas y Pesos:** El bloque de construcci贸n principal es la **capa**, que implementa una transformaci贸n de datos definida por sus **pesos** o **par谩metros**. El aprendizaje consiste en encontrar el conjunto 贸ptimo de valores para estos pesos en cada capa.
* **Algoritmo Central:** La **Retropropagaci贸n (Backpropagation)** es el algoritmo que calcula el gradiente de la funci贸n de p茅rdida con respecto a los pesos. Esto permite al **Optimizador** ajustar los pesos en la direcci贸n que minimiza la p茅rdida.
* **Bucle de Entrenamiento:** Los pesos iniciales son aleatorios. A trav茅s de este bucle iterativo, se ajustan progresivamente para acercarse al resultado deseado.

---

###  Panorama de Algoritmos (Machine Learning Cl谩sico)

El cap铆tulo tambi茅n hace menci贸n a otros algoritmos de ML fuera del DL:

* **Modelos Probabil铆sticos:** Muchos modelos son estad铆sticos, como el clasificador **Naive Bayes** y la **regresi贸n log铆stica**, basados en el Teorema de Bayes.
* **M茅todos Kernel:** Algoritmos como **SVM (Support Vector Machine)** buscan un hiperplano 贸ptimo para la separaci贸n de clases. Son muy efectivos pero dif铆ciles de escalar para grandes vol煤menes de datos.
* **rboles de Decisiones y Potenciaci贸n de Gradiente:**
    * **Random Forest** es considerado el segundo mejor algoritmo "generalista" para la mayor铆a de los problemas de ML cl谩sico.
    * La **Potenciaci贸n de Gradiente (Gradient Boosting Machine)** es a menudo el mejor algoritmo para tratar **datos tabulares** o no perceptuales.
* **Redes Neuronales Convolucionales (ConvNets):** Son el algoritmo de referencia en tareas de **visi贸n por ordenador** (ej. ImageNet para clasificaci贸n de im谩genes de alta resoluci贸n).

---

##  Conceptos T茅cnicos de la Red Neuronal

### Flujo de Trabajo

El flujo de trabajo est谩ndar de una red neuronal es:
1.  Proporcionar los datos de entrenamiento.
2.  La red aprende la asociaci贸n entre los datos y las etiquetas.
3.  Se solicitan **predicciones** para datos de prueba (`test_images`), y se comprueba su precisi贸n contra las etiquetas de prueba (`test_labels`).

### Bloque de Construcci贸n: La Capa

El Deep Learning es un proceso de **destilaci贸n o filtrado** que encadena capas. Cada capa refina los datos, extrayendo representaciones cada vez m谩s complejas y significativas.

### 锔 Proceso de Compilaci贸n

Para configurar la red antes del entrenamiento, se necesitan tres elementos clave:

1.  **Funci贸n de P茅rdida (Loss Function):** Mide la precisi贸n de la red y el rendimiento.
2.  **Optimizador:** El mecanismo que implementa la retropropagaci贸n para actualizar los pesos de la red.
3.  **M茅tricas:** Valores a monitorear durante el entrenamiento y la prueba (e.g., la **precisi贸n/accuracy** media).

###  Representaci贸n de Datos: Tensores

La estructura de datos fundamental en Machine Learning es el **tensor**, un contenedor para datos almacenados en matrices NumPy multidimensionales.

| Rango | Nombre | Descripci贸n | Uso Com煤n |
| :---: | :---: | :--- | :--- |
| **0D** | Escalar | Un solo n煤mero. | Temperatura, edad. |
| **1D** | Vector | Una cadena de n煤meros. | Series de tiempo, caracter铆sticas. |
| **2D** | Matriz | Una cadena de vectores (filas y columnas). | Datos tabulares: `(muestras, caracter铆sticas)`. |
| **3D** | Tensor 3D | Una cadena de matrices. | Datos secuenciales: `(muestras, tiempo, caracter铆sticas)`. |
| **4D** | Tensor 4D | Im谩genes. | `(muestras, altura, anchura, canales)`. |
| **5D** | Tensor 5D | Video. | `(muestras, fotograma, altura, anchura, canales)`. |

* **Atributos Clave:**
    * `x.ndim`: El **rango** (n煤mero de ejes o dimensiones).
    * `shape`: La forma del tensor, una tupla que indica el tama帽o de cada eje (e.g., `(60000, 28, 28)`).
    * `dtype`: El tipo de datos que contiene el tensor (ej. `float32`).

### Lotes de Datos

En Deep Learning, los datos se procesan en **lotes (batches)** peque帽os (subconjuntos `x.[a:b]`) en lugar de alimentar la red con todo el conjunto de datos a la vez. El **eje 0** del tensor siempre corresponde al **eje de la muestra** o dimensi贸n del lote.
